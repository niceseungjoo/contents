# 퍼셉트론이란 무엇인가

### 퍼셉트론은 모든 모델들의 가장 기반이 되는 기술
- **MLP, CNN, Transformer**가 각각 스포츠카, 승용차, 버스라면 퍼셉트론은 '바퀴' 와 같음

### 가장 기초적인 퍼셉트론의 기능방식
**입력: x = [x₁, x₂, x₃, ..., xₙ] (1차원 실수 벡터)**
**가중치: w = [w₁, w₂, w₃, ..., wₙ] (1차원 실수 벡터)**
**편향: b (스칼라)**
**연산: Σ(wᵢ × xᵢ) + b = w₁x₁ + w₂x₂ + ... + wₙxₙ + b**

*예를 들어보면*

직선 방정식: **`ax₁ + bx₂ + c = 0`**
- **a, b**는 **가중치**
- **c**는 **편향**
- x값 변수 두 개로 이루어지는 2차원 선형 분류
  - **a, b**는 **기울기**, **c**는 **y절편**
  - 이 출력값이 0 이상 or 이하 -> **이진 분류**

### 한계
**XOR문제**
-> 해결을 위해 **MLP**가 등장하게 됨


# MLP의 등장

### 구조
**입력층**
- 각 뉴런마다 원래의 데이터를 수치화한 **특성값**으로서 실수 형태로 하나씩 들어감
- 이미지 기준 총 8개의 특성값으로 데이터가 나타내어진 것

**은닉층**
- 여러개의 층이, 층마다 여러개의 뉴런이 존재
- 층이 깊어질수록 더 구체적이고 자세한 특성 학습
- 뉴런 하나하나가 아까 봤던 퍼셉트론의 기능을 함
    - 하나의 뉴런 안에는 이전 층에서의 활성화 값의 갯수 만큼 가중치를 각각 가지고있음
      ex)
      뉴런 1: z₁₁ = x₁×w₁₁₁ + x₂×w₁₁₂ + x₃×w₁₁₃ + b₁₁
      = 2.0×0.5 + (-1.5)×(-0.3) + 0.8×0.2 + 0.1 = 1.71
      h₁₁ = ReLU(1.71) = 1.71

      뉴런 2: z₁₂ = 2.0×(-0.2) + (-1.5)×0.4 + 0.8×0.1 + (-0.1) = -0.58
      h₁₂ = ReLU(-0.58) = 0.0

      뉴런 3: z₁₃ = 2.0×0.3 + (-1.5)×0.1 + 0.8×(-0.4) + 0.2 = 0.33
      h₁₃ = ReLU(0.33) = 0.33

      뉴런 4: z₁₄ = 2.0×0.1 + (-1.5)×(-0.2) + 0.8×0.3 + 0.0 = 0.74
      h₁₄ = ReLU(0.74) = 0.74

        - 전 층에서의 입력값이 4개 > 뉴런 하나마다 서로 다른 가중치가 4개씩
            - 각각의 입력값 마다 가중치가 곱해져서 편향이 더해지고, 이 가중합이 다음층으로 갈 때의 그 뉴런의 활성화값이 된다..
        - ReLU는 **활성화함수**

**출력층**
- 최종 예측값을 알아내는
- 분류문제라면 분류 클래스만큼 뉴런 수를 가짐(10클래스의 이미지분류라면 출력층의 뉴런은 10개)


### 특징: 활성화함수를 통한 비선형성의 추가
**활성화 함수**가 없다면
-> 가중합의 선형 변환의 연속이 되어 복잡한 패턴에 대한 학습이 불가능함

**ReLU**

**수식**: **f(x) = max(0, x)**
- x가 양수면 x를 그대로 출력
- x가 0 이하면 0을 출력

**특징**
- **은닉층에서 주로 사용**: 현재 가장 표준적인 선택
- **CNN, RNN 등 모든 신경망 구조**에서 널리 활용
- **출력층에서는 거의 사용하지 않음** - 음수 값을 출력할 수 없기 때문

**Sigmoid**

**수식: σ(x) = 1 / (1 + e^(-x))**
- 지수계산 때문에 연산비용이 높음

**특징**
- 출력값의 범위가 정해져있음(0-1) - 수치적 안정성
- 이진분류 문제에 용이하게 사용됨 - 0~1의 값을 확률로서 해석이 가능
- 은닉층에선 많이 안쓰임
